# Preven√ß√£o √† fraude - Tribo Riscos&Banking

## Squad Preven√ß√£o √† fraude
A squad Preven√ß√£o √† Fraude √© respons√°vel por processar dados e gerar insights valiosos atrav√©s de dashboards que ajudam na identifica√ß√£o de fraudes, entendimento aprofundado do que est√° ocorrendo e melhoria nas regras de preven√ß√£o. Atendemos aos gestores das empresas Palmeiras, Carmen Steffens e Leroy Merlin, e auxiliamos em processos da tribo de neg√≥cios.

## O que estamos fazendo?
Migra√ß√£o da solu√ß√£o de processamento de dados de Pentaho para AirFlow e Spark nos neg√≥cios White Label, CallCenter, entre outros. 

## Vantagens da migra√ß√£o
- Adequa√ß√£o √†s ferramentas utilizadas pela PEFISA.
- Redu√ß√£o no tempo de processamento de dados
- Redu√ß√£o no tempo para gera√ß√£o de insights e identifica√ß√£o de fraudes
- Automa√ß√£o de processos, com agendamento das pipelines de processamento no AirFlow

## Fluxo de Trabalho

### Atual
```mermaid
  sequenceDiagram
    participant Channel as ‚úâÔ∏è Canais de comunica√ß√£o
    participant Programmer as üë®‚Äçüíª Programador
    participant Citrix as üíª Ambiente Citrix
    participant Zeppelin as üéà Ambiente Zeppelin
    participant Spark as ‚öôÔ∏è Motor Spark
    participant Hue as üíæ Sistema Hue
    participant CSVViewer as üîç Visualizador de CSV

    Channel->>Programmer: Apresentar demandas
    Programmer->>Citrix: Conectar ao Ambiente Citrix
    Citrix->>Zeppelin: Acessar Ambiente Zeppelin
    Programmer->>Zeppelin: Desenvolver c√≥digo PySpark
    Zeppelin->>Spark: Executar jobs Spark
    Spark->>Hue: Armazenar dados processados
    Programmer->>Zeppelin: Analisar dados armazenados
    Zeppelin->>Hue: Recuperar dados
    Zeppelin->>Programmer: Exibir resultados da an√°lise
    Programmer->>Zeppelin: Exportar dados tratados para CSV
    Zeppelin->>CSVViewer: Abrir CSV exportado
    CSVViewer->>Programmer: Visualizar dados
```


### Em implementa√ß√£o
```mermaid 
    sequenceDiagram
    participant Channel as ‚úâÔ∏è Canais de comunica√ß√£o
    participant Programmer as üë®‚Äçüíª Programador
    participant Citrix as üíª Ambiente Citrix
    participant Zeppelin as üéà Ambiente Zeppelin
    participant Bitbucket as üóÑÔ∏è Bitbucket
    participant ComposerDev as üõ†Ô∏è Composer (Desenvolvimento)
    participant ComposerProd as üöÄ Composer (Produ√ß√£o)

    Channel->>Programmer: Apresentar demandas
    Programmer->>Citrix: Conectar ao Ambiente Citrix
    Citrix->>Zeppelin: Acessar Ambiente Zeppelin
    Programmer->>Zeppelin: Desenvolver e testar c√≥digo PySpark
    Zeppelin->>Bitbucket: Enviar c√≥digo para gest√£o e controle de vers√£o
    Bitbucket->>ComposerDev: Implementar c√≥digo em desenvolvimento
    ComposerDev->>Programmer: Testar e validar pipeline
    ComposerDev->>ComposerProd: Implementar pipeline em produ√ß√£o
```


## Ambiente de Trabalho e Ferramentas
- **Ferramentas:**
  - **Citrix:** Para se conectar aos softwares da Pernambucanas (Impala, Hue) e 2RP (Zeppelin)
  - **Impala e Hue:** Para acessar e consultar os dados
  - **Zeppelin:** Para desenvolvimento e execu√ß√£o de c√≥digo PySpark, conectado ao Impala e Hue
  - **Spark:** Instalado no ambiente Zeppelin para processamento de dados
  - **Airflow**: Para orquestra√ß√£o de pipelines

## Exemplo de Processo de Tratamento de Dados

### Passos da Demonstra√ß√£o ao Vivo:
- **1 - Iniciar a SparkSession:**
    ```python
    from pyspark.sql import SparkSession
    spark = SparkSession.builder \
        .appName("Exemplo de Tratamento de Dados") \
        .getOrCreate()
    ```

-  **2 - Ler dados de uma tabela do Impala:**
    ```python
    df = spark.read.format("jdbc") \
        .option("url", "jdbc:impala://<impala_host>:<impala_port>/<database>") \
        .option("dbtable", "<tabela>") \
        .option("user", "<usuario>") \
        .option("password", "<senha>") \
        .load()
    ```

- **3 - Selecionar colunas importantes:**
    ```python
    df_selecionado = df.select("coluna1", "coluna2", "coluna3")
    ```

- **4 - Renomear colunas:**
    ```python
    df_renomeado = df_selecionado.withColumnRenamed("coluna1", "nova_coluna1")
    ```

- **5 - Converter tipos de dados:**
    ```python
    df_convertido = df_renomeado.withColumn("nova_coluna1", col("nova_coluna1").cast("Integer"))
    ```

- **6 - Mostrar o resultado final:**
    ```python
    df_convertido.show()
    ```


### Pr√≥ximos Passos
  - Aderir ao Bitbucket para versionamento de c√≥digo
  - Desenvolver pipeline de CI e CD
  - Implementar pipelines de processamento de dados no Airflow hospedado no GCP para orquestra√ß√£o de workflows
  - Migrar todos os fluxos de Pentarro para Airflow